import os
import matplotlib.pylab as plt
from tensorflow.keras import models
from tensorflow.keras import layers

#import shutil

original_dataset_dir = 'C:\\Users\\t659699\\Tools\\Kaggle_Original_Data\\train'

base_dir = 'C:\\Users\\t659699\\Tools\\Kaggle_Original_Data\\cats_and_dogs_small'
# os.mkdir(base_dir)
#
train_dir = os.path.join(base_dir, 'train')
# os.mkdir(train_dir)
#
validation_dir = os.path.join(base_dir, 'validation')
# os.mkdir(validation_dir)
#
test_dir = os.path.join(base_dir, 'test')
# os.mkdir(test_dir)
#
train_cats_dir = os.path.join(train_dir, 'cats')
# os.mkdir(train_cats_dir)
#
train_dogs_dir = os.path.join(train_dir, 'dogs')
# os.mkdir(train_dogs_dir)
#
validation_cats_dir = os.path.join(validation_dir, 'cats')
# os.mkdir(validation_cats_dir)
#
validation_dogs_dir = os.path.join(validation_dir, 'dogs')
# os.mkdir(validation_dogs_dir)
#
test_cats_dir = os.path.join(test_dir, 'cats')
# os.mkdir(test_cats_dir)
#
test_dogs_dir = os.path.join(test_dir, 'dogs')
# os.mkdir(test_dogs_dir)
#
# # copy first 1000 cats images to train_cats
#
# fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]  #format wstawia w {} 'i' do stringa
# for fname in fnames:
#     src = os.path.join(original_dataset_dir, fname)
#     dst = os.path.join(train_cats_dir, fname)
#     shutil.copyfile(src, dst)
#
# # copy next 500 cats to validation_path
#
# fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]
# for fname in fnames:
#     src = os.path.join(original_dataset_dir, fname)
#     dst = os.path.join(validation_cats_dir, fname)
#     shutil.copyfile(src, dst)
#
# # copy next 500 cats to test_cats_path
#
# fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]
# for fname in fnames:
#     src = os.path.join(original_dataset_dir, fname)
#     dst = os.path.join(test_cats_dir, fname)
#     shutil.copyfile(src, dst)
#
# # copy 1000 dogs to train_path
#
# fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]  # format wstawia w {} 'i' do stringa
# for fname in fnames:
#     src = os.path.join(original_dataset_dir, fname)
#     dst = os.path.join(train_dogs_dir, fname)
#     shutil.copyfile(src, dst)
#
# # copy next 500 dogs to validation_path
#
# fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]
# for fname in fnames:
#     src = os.path.join(original_dataset_dir, fname)
#     dst = os.path.join(validation_dogs_dir, fname)
#     shutil.copyfile(src, dst)
#
# # copy next 500 dogs to test_cats_path
#
# fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]
# for fname in fnames:
#     src = os.path.join(original_dataset_dir, fname)
#     dst = os.path.join(test_dogs_dir, fname)
#     shutil.copyfile(src, dst)

# # Creating model


model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))


model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

print(model.summary())

# Data preprocessing (jpg->np.array) - za pomocą generatora

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# train_dataGen = ImageDataGenerator(rescale = 1./255)
# test_dataGen = ImageDataGenerator(rescale = 1./255)

dataGen = ImageDataGenerator(rescale=1. / 255)

train_generator = dataGen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary'  # binarne labele
)

validation_generator = dataGen.flow_from_directory(
    validation_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary'  # binarne labele
)

# Uwaga, generator niewie kiedy jest koniec danych i będzie je generował bez końca
# dla model.fit generator steps_per_epochs (lub validation_steps) = ileDanych/generator.batch_size

history = model.fit_generator(train_generator, steps_per_epoch=100, epochs=30, validation_data=validation_generator,
                              validation_steps=50)

# acc = history.history['acc']
# val_acc = history.history['val_acc']
# loss = history.history['loss']
# val_loss = history.history['val_loss']
# steps = range(1, len(acc) + 1)
#
#
# plt.plot(acc, steps, 'bo', label='train acc')
# plt.plot(val_acc, steps, 'b', label='val acc')
# plt.legend()
#
# plt.figure()
# plt.plot(loss, steps, 'bo', label='train loss')
# plt.plot(val_loss, 'b', label='val loss')
# plt.legend()
#
# plt.show()
#
#
model.save("my_catdog_model")

